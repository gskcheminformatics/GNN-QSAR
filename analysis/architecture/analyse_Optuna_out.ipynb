{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from rdkit.Chem import Crippen\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from rdkit.Chem.AllChem import GetMorganGenerator\n",
    "from rdkit import DataStructs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-pricing",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## 0. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "herg_df = pd.read_csv(<herg_file>, sep=\"\\t\")\n",
    "clogd_df = pd.read_csv(<clogd_file>, sep=\"\\t\")\n",
    "cad_df = pd.read_csv(<cad_file>, sep=\"\\t\")\n",
    "cyp3a4_df = pd.read_csv(<cyp3a4_file>, sep=\"\\t\")\n",
    "\n",
    "lck_df = pd.read_csv(<lck_file>, sep=\"\\t\")\n",
    "jak2_df = pd.read_csv(<jak2_file>, sep=\"\\t\")\n",
    "\n",
    "ppara_df = pd.read_csv(<ppara_file>, sep=\"\\t\")\n",
    "ppard_df = pd.read_csv(<ppard_file>, sep=\"\\t\")\n",
    "pparg_df = pd.read_csv(<pparg_file>, sep=\"\\t\")\n",
    "\n",
    "\n",
    "print('hERG size:',len([i for i in herg_df['mol'] if i is not None]))\n",
    "print('ppara size:',len([i for i in ppara_df['mol'] if i is not None]))\n",
    "print('ppard size:',len([i for i in ppard_df['mol'] if i is not None]))\n",
    "print('pparg size:',len([i for i in pparg_df['mol'] if i is not None]))\n",
    "print('CAD size:',len([i for i in cad_df['mol'] if i is not None]))\n",
    "print('LCK size:',len([i for i in lck_df['mol'] if i is not None]))\n",
    "\n",
    "print('ChromLogD size:',len([i for i in clogd_df['mol'] if i is not None]))\n",
    "\n",
    "print('JAK2 size:',len([i for i in jak2_df['mol'] if i is not None]))\n",
    "\n",
    "print('CYP3A4 size:',len([i for i in cyp3a4_df['mol'] if i is not None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature importances\n",
    "study_obj = optuna.create_study(storage=<optuna_run_db>, study_name=<optuna_study_name>, load_if_exists=True)\n",
    "optuna.visualization.plot_param_importances(\n",
    "        study_obj, target=lambda t: t.duration.total_seconds(), target_name=\"value\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-encounter",
   "metadata": {},
   "source": [
    "## 1. Pre-process data end points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-spirituality",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_full_endpoint_hparam_df(study_endpoints, database_path, group_by_layer_type=False, with_lr_cutoff=False, no_top5_bottom5=False):\n",
    "    \"\"\"\n",
    "    Creates DataFrame of top5 and bottom5 values for each study endpoint given\n",
    "    \n",
    "    Inputs\n",
    "    -------\n",
    "    study_endpoints: dictionary\n",
    "        Keys are dataset names and values are study strings e.g. {\"hERG\": \"herg_study_new_hparam_test\", \"ChromLogD\": \"clogd_study_new_hparam_test\"}\n",
    "    database_path: string\n",
    "        Path where SQLite databases are stored for each study e.g. \"sqlite:////hpc/mydata/upt/ns833749/pytorch_geometric/graphgym/\"\n",
    "    group_by_layer_type: bool\n",
    "        True if top5 and bottom5 per layer type rather than across all parameters\n",
    "    with_lr_cutoff: bool\n",
    "        True if implementing learning rate cut-off for identification of top5 and bottom5 hyperparameters\n",
    "    no_top5_bottom5: bool\n",
    "        True if only interested in full DataFrame\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    full_final_df: DataFrame\n",
    "        Pandas DataFrame containing all studies given for study_endpoints\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    full_final_df = pd.DataFrame()\n",
    "    \n",
    "    for study in study_endpoints.keys():\n",
    "        study_obj = optuna.create_study(storage=database_path+study_endpoints[study]+\".db\", study_name=study_endpoints[study], load_if_exists=True)\n",
    "        \n",
    "        study_df = study_obj.trials_dataframe()\n",
    "        \n",
    "        study_df = study_df[study_df[\"state\"] == \"COMPLETE\"]\n",
    "        study_df[\"dataset\"] = study\n",
    "        \n",
    "        if no_top5_bottom5:\n",
    "            full_final_df = pd.concat([full_final_df, study_df])\n",
    "            \n",
    "        else:\n",
    "            if group_by_layer_type:\n",
    "                study_df_layers = list(study_df[\"params_layer_type\"].unique())\n",
    "\n",
    "                top5 = pd.DataFrame()\n",
    "                bottom5 = pd.DataFrame()\n",
    "\n",
    "                for layer_type in study_df_layers:\n",
    "                    if with_lr_cutoff:\n",
    "                        bottom5_per_layer = study_df[(study_df[\"params_layer_type\"] == layer_type) & (study_df[\"params_base_lr\"] >= 0.0001)].sort_values('value')[-5:]\n",
    "                        top5_per_layer = study_df[(study_df[\"params_layer_type\"] == layer_type) & (study_df[\"params_base_lr\"] >= 0.0001)].sort_values('value')[:5]\n",
    "                    else:\n",
    "                        bottom5_per_layer = study_df[study_df[\"params_layer_type\"] == layer_type].sort_values('value')[-5:]\n",
    "                        top5_per_layer = study_df[study_df[\"params_layer_type\"] == layer_type].sort_values('value')[:5]\n",
    "                        \n",
    "                    if len(top5_per_layer) == 5:\n",
    "                        top5_per_layer[\"rank\"] = [i+1 for i in range(5)]\n",
    "                        top5_per_layer[\"category\"] = \"top5\"\n",
    "\n",
    "                    if len(bottom5_per_layer) == 5:\n",
    "                        bottom5_per_layer[\"rank\"] = [-(i+1) for i in range(5)][::-1]\n",
    "                        bottom5_per_layer[\"category\"] = \"bottom5\"\n",
    "\n",
    "                    if len(top5_per_layer) == 5 and len(bottom5_per_layer) == 5:\n",
    "                        top5 = pd.concat([top5, top5_per_layer])\n",
    "                        bottom5 = pd.concat([bottom5, bottom5_per_layer])\n",
    "\n",
    "            else:\n",
    "                if with_lr_cutoff:\n",
    "                    bottom5 = study_df[study_df[\"params_base_lr\"] >= 0.0001].sort_values('value')[-5:]\n",
    "                    top5 = study_df[study_df[\"params_base_lr\"] >= 0.0001].sort_values('value')[:5]\n",
    "                else:\n",
    "                    bottom5 = study_df.sort_values('value')[-5:]\n",
    "                    top5 = study_df.sort_values('value')[:5]\n",
    "\n",
    "                bottom5[\"rank\"] = [-(i+1) for i in range(5)][::-1]\n",
    "                bottom5[\"category\"] = \"bottom5\"\n",
    "                \n",
    "                top5[\"rank\"] = [i+1 for i in range(5)]\n",
    "                top5[\"category\"] = \"top5\"\n",
    "\n",
    "            study_df_concat = pd.concat([top5, bottom5])\n",
    "            full_final_df = pd.concat([full_final_df, study_df_concat])\n",
    "        \n",
    "    return full_final_df\n",
    "\n",
    "full_df_no_top_bottom = create_full_endpoint_hparam_df(study_endpoints={\"hERG\": <herg_study>, \n",
    "                                                \"ChromLogD\": <clogd_study>,\n",
    "                                                \"CAD\": <cad_study>,\n",
    "                                                \"CYP3A4\": <cyp3a4_study>,\n",
    "                                               \"LCK\": <lck_study>,\n",
    "                                               \"JAK2\": <jak2_study>,\n",
    "                                               \"PPAR_A\": <ppara_study>,\n",
    "                                               \"PPAR_D\": <ppard_study>,\n",
    "                                               \"PPAR_G\": <pparg_study>},\n",
    "                              database_path=<optuna_db_location>,\n",
    "                                                  no_top5_bottom5=True)\n",
    "        \n",
    "full_df_endpoints = create_full_endpoint_hparam_df(study_endpoints={\"hERG\": <herg_study>, \n",
    "                                                \"ChromLogD\": <clogd_study>,\n",
    "                                                \"CAD\": <cad_study>,\n",
    "                                                \"CYP3A4\": <cyp3a4_study>,\n",
    "                                               \"LCK\": <lck_study>,\n",
    "                                               \"JAK2\": <jak2_study>,\n",
    "                                               \"PPAR_A\": <ppara_study>,\n",
    "                                               \"PPAR_D\": <ppard_study>,\n",
    "                                               \"PPAR_G\": <pparg_study>},\n",
    "                              database_path=<optuna_db_location>,\n",
    "                                                  group_by_layer_type=False)\n",
    "\n",
    "full_df_endpoints_with_lr_cutoff_bottom5 = create_full_endpoint_hparam_df(study_endpoints={\"hERG\": <herg_study>, \n",
    "                                                \"ChromLogD\": <clogd_study>,\n",
    "                                                \"CAD\": <cad_study>,\n",
    "                                                \"CYP3A4\": <cyp3a4_study>,\n",
    "                                               \"LCK\": <lck_study>,\n",
    "                                               \"JAK2\": <jak2_study>,\n",
    "                                               \"PPAR_A\": <ppara_study>,\n",
    "                                               \"PPAR_D\": <ppard_study>,\n",
    "                                               \"PPAR_G\": <pparg_study>},\n",
    "                              database_path=<optuna_db_location>,\n",
    "                                                  group_by_layer_type=False, with_lr_cutoff=True)\n",
    "\n",
    "full_df_endpoints_per_layer = create_full_endpoint_hparam_df(study_endpoints={\"hERG\": <herg_study>, \n",
    "                                                \"ChromLogD\": <clogd_study>,\n",
    "                                                \"CAD\": <cad_study>,\n",
    "                                                \"CYP3A4\": <cyp3a4_study>,\n",
    "                                               \"LCK\": <lck_study>,\n",
    "                                               \"JAK2\": <jak2_study>,\n",
    "                                               \"PPAR_A\": <ppara_study>,\n",
    "                                               \"PPAR_D\": <ppard_study>,\n",
    "                                               \"PPAR_G\": <pparg_study>},\n",
    "                              database_path=<optuna_db_location>,\n",
    "                                                  group_by_layer_type=True)\n",
    "\n",
    "full_df_endpoints_per_layer_with_lr_cutoff_bottom5 = create_full_endpoint_hparam_df(study_endpoints={\"hERG\": <herg_study>, \n",
    "                                                \"ChromLogD\": <clogd_study>,\n",
    "                                                \"CAD\": <cad_study>,\n",
    "                                                \"CYP3A4\": <cyp3a4_study>,\n",
    "                                               \"LCK\": <lck_study>,\n",
    "                                               \"JAK2\": <jak2_study>,\n",
    "                                               \"PPAR_A\": <ppara_study>,\n",
    "                                               \"PPAR_D\": <ppard_study>,\n",
    "                                               \"PPAR_G\": <pparg_study>},\n",
    "                              database_path=<optuna_db_location>,\n",
    "                                                  group_by_layer_type=True, with_lr_cutoff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom5_data_lr = full_df_endpoints[(full_df_endpoints[\"category\"] == 'bottom5') & (full_df_endpoints[\"params_base_lr\"] <= 0.0001)][\"params_base_lr\"]\n",
    "top5_data_lr = full_df_endpoints[(full_df_endpoints[\"category\"] == 'top5') & (full_df_endpoints[\"params_base_lr\"] <= 0.0001)][\"params_base_lr\"]\n",
    "\n",
    "bottom5_data_lr.hist(bins=10, alpha=0.5, label='bottom5', weights=np.ones(len(bottom5_data_lr)) / len(bottom5_data_lr))\n",
    "top5_data_lr.hist(bins=10, alpha=0.5, label='top5', weights=np.ones(len(top5_data_lr)) / len(top5_data_lr))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Fraction of data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-purchase",
   "metadata": {},
   "source": [
    "## 2. Analyse hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_colour(df):\n",
    "    if len(df) <= 10000:\n",
    "        colour = 'red'\n",
    "    if len(df) > 10000 and len(df) <= 20000:\n",
    "        colour = 'orangered'\n",
    "    if len(df) > 20000 and len(df) <= 30000:\n",
    "        colour = 'darkorange'\n",
    "    if len(df) > 30000 and len(df) <= 40000:\n",
    "        colour = 'orange'\n",
    "    if len(df) > 40000 and len(df) <= 100000:\n",
    "        colour = 'gold'\n",
    "    if len(df) > 100000 and len(df) <= 200000:\n",
    "        colour = 'yellow'\n",
    "    if len(df) > 200000 and len(df) <= 300000:\n",
    "        colour = 'greenyellow'\n",
    "    if len(df) > 300000:\n",
    "        colour = 'green'\n",
    "            \n",
    "    return colour\n",
    "\n",
    "datasets = {\"hERG\": herg_df, \n",
    "            \"ChromLogD\": clogd_df,\n",
    "            \"CAD\": cad_df,\n",
    "            \"CYP3A4\": cyp3a4_df,\n",
    "           \"LCK\": lck_df,\n",
    "           \"JAK2\": jak2_df,\n",
    "           \"PPAR_A\": ppara_df,\n",
    "           \"PPAR_D\": ppard_df,\n",
    "           \"PPAR_G\": pparg_df}\n",
    "\n",
    "custom_palette = {}\n",
    "for dataset in datasets:\n",
    "    colour = custom_colour(datasets[dataset])\n",
    "    \n",
    "    custom_palette[dataset] = colour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import to_rgb\n",
    "\n",
    "def split_violin_plot(df, param_name, xlim=None):\n",
    "    \"\"\"\n",
    "    Generates split violin plot given DataFrame of studies and parameter name\n",
    "    \n",
    "    df: DataFrame\n",
    "        DataFrame of studies\n",
    "    param_name: string\n",
    "        Name of parameter to plot\n",
    "    ylim: list\n",
    "        Lower and upper bounds of y limit e.g. [-0.005,0.005]\n",
    "    \"\"\"\n",
    "    \n",
    "    datasets = df['dataset'].unique()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    ax = sns.violinplot(data=df, y=\"dataset\", x=param_name, hue=\"category\",\n",
    "                   split=True, inner=\"quart\", fill=False, palette=['.4', '.7'])\n",
    "        \n",
    "    colors = [i for i in custom_palette.values()]\n",
    "    for ind, violin in enumerate(ax.findobj(PolyCollection)):\n",
    "        rgb = to_rgb(colors[ind // 2])\n",
    "        if ind % 2 != 0:\n",
    "            rgb = 0.5 + 0.5 * np.array(rgb)\n",
    "        violin.set_facecolor(rgb)\n",
    "\n",
    "    if xlim:\n",
    "        ax.set_xlim(xlim[0],xlim[1])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def split_violin_plot_per_layer_type(df, param_name, xlim=None):\n",
    "    \"\"\"\n",
    "    Generates split violin plot per layer type given DataFrame of studies and a parameter name\n",
    "    \n",
    "    df: DataFrame\n",
    "        DataFrame of studies\n",
    "    dataset: string\n",
    "        Name of dataset\n",
    "    param_name: string\n",
    "        Name of parameter to plot\n",
    "    ylim: list\n",
    "        Lower and upper bounds of y limit e.g. [-0.005,0.005]\n",
    "    \"\"\"\n",
    "    \n",
    "    for dataset in list(df[\"dataset\"].unique()):\n",
    "        df_dataset = df[df[\"dataset\"] == dataset]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "        ax = sns.violinplot(data=df_dataset, y=\"params_layer_type\", x=param_name, hue=\"category\",\n",
    "                       split=True, inner=\"quart\", fill=False, palette=['.4', '.7'])\n",
    "                       #palette={\"top5\": \"g\", \"bottom5\": \".35\"})\n",
    "\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim[0],xlim[1])\n",
    "\n",
    "        plt.title(dataset)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-genius",
   "metadata": {},
   "source": [
    "### Plots per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-hurricane",
   "metadata": {},
   "source": [
    "#### 1. Without cutoff for lr bottom5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-constant",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params \"params_base_lr\", \"params_dropout\", \"params_layers_mp\", \"params_lr_decay\", \"params_dim_inner\", \"params_weight_decay\"\n",
    "\n",
    "split_violin_plot_per_layer_type(df=full_df_endpoints_per_layer, param_name=\"params_layers_mp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-editor",
   "metadata": {},
   "source": [
    "#### 2. With lr cutoff bottom5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-fault",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params \"params_base_lr\", \"params_dropout\", \"params_layers_mp\", \"params_lr_decay\", \"params_dim_inner\", \"params_weight_decay\"\n",
    "\n",
    "split_violin_plot_per_layer_type(df=full_df_endpoints_per_layer_with_lr_cutoff_bottom5, param_name=\"params_layers_mp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-sculpture",
   "metadata": {},
   "source": [
    "### Plots per dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-missouri",
   "metadata": {},
   "source": [
    "#### 1. Without lr cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-puppy",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "continuous_hparams = [\"params_base_lr\", \"params_dropout\", \"params_layers_mp\", \"params_lr_decay\", \"params_dim_inner\", \"params_weight_decay\"]\n",
    "\n",
    "######################################################################################################################################################################\n",
    "# COLOURS (redder means less data points):\n",
    "# Datapoints: <= 10000: red, <= 20000: orangered, <= 30000: darkorange, <= 40000: orange, <= 100000: gold, <= 200000: yellow, <= 300000: greenyellow, > 300000: green\n",
    "######################################################################################################################################################################\n",
    "\n",
    "for param in continuous_hparams:\n",
    "    if param == \"params_base_lr\":\n",
    "        xlim = [-0.005,0.005]\n",
    "    else:\n",
    "        xlim = None\n",
    "        \n",
    "    split_violin_plot(df=full_df_endpoints, param_name=param, xlim=xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-cable",
   "metadata": {},
   "source": [
    "#### 2. With lr cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-pocket",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "continuous_hparams = [\"params_base_lr\", \"params_dropout\", \"params_layers_mp\", \"params_lr_decay\", \"params_dim_inner\", \"params_weight_decay\"]\n",
    "\n",
    "######################################################################################################################################################################\n",
    "# COLOURS (redder means less data points):\n",
    "# Datapoints: <= 10000: red, <= 20000: orangered, <= 30000: darkorange, <= 40000: orange, <= 100000: gold, <= 200000: yellow, <= 300000: greenyellow, > 300000: green\n",
    "######################################################################################################################################################################\n",
    "\n",
    "for param in continuous_hparams:\n",
    "    split_violin_plot(df=full_df_endpoints_with_lr_cutoff_bottom5, param_name=param, xlim=xlim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_hparams = [\"params_base_lr\", \"params_dropout\", \"params_layers_mp\", \"params_lr_decay\", \"params_dim_inner\", \"params_weight_decay\"]\n",
    "full_df_cont_hparams = full_df_no_top_bottom[['number', 'value', 'params_base_lr', 'params_dim_inner', 'params_dropout', 'params_layers_mp', 'params_layers_post_mp', 'params_layers_pre_mp', 'params_lr_decay', 'params_weight_decay', 'dataset']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT LR CUTOFF, ALL DATA\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Params most correlated to output value: layers_post_mp, layers_pre_mp, lr_decay, dim_inner\n",
    "# Params most correlated to learning rate: layers_mp, dropout, weight_decay\n",
    "\n",
    "corr = full_df_cont_hparams.corr()\n",
    "sns.heatmap(corr, cmap='icefire', annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT LR CUTOFF, TOP5/BOTTOM5\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Params most correlated to output value: layers_post_mp, layers_mp, dropout\n",
    "# Params most correlated to learning rate: layers_post_mp, layers_pre_mp, dropout\n",
    "\n",
    "corr = full_df_endpoints[['number', 'value', 'params_base_lr', 'params_dim_inner', 'params_dropout', 'params_layers_mp', 'params_layers_post_mp', 'params_layers_pre_mp', 'params_lr_decay', 'params_weight_decay', 'dataset']].corr()\n",
    "sns.heatmap(corr, cmap='icefire', annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH LR CUTOFF\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Params most correlated to output value: layers_mp, layers_post_mp, weight_decay\n",
    "# Params most correlated to learning rate: layers_pre_mp, layers_post_mp, dropout, dim_inner\n",
    "\n",
    "corr = full_df_endpoints_with_lr_cutoff_bottom5[['number', 'value', 'params_base_lr', 'params_dim_inner', 'params_dropout', 'params_layers_mp', 'params_layers_post_mp', 'params_layers_pre_mp', 'params_lr_decay', 'params_weight_decay', 'dataset']].corr()\n",
    "sns.heatmap(corr, cmap='icefire', annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-cosmetic",
   "metadata": {},
   "source": [
    "### Are important parameters correlated to dataset size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {}\n",
    "\n",
    "for dataset in full_df_no_top_bottom['dataset'].unique():\n",
    "    dataset_sizes[dataset] = len(full_df_no_top_bottom[full_df_no_top_bottom['dataset'] == dataset])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_endpoints_with_lr_cutoff_bottom5['dataset_size'] = full_df_endpoints_with_lr_cutoff_bottom5['dataset'].apply(lambda x: [v for k, v in dataset_sizes.items() if k in x][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH LR CUTOFF (for top5)\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Dataset size seems most correlated to learning rate\n",
    "full_df_endpoints_with_lr_cutoff_bottom5_top5only = full_df_endpoints_with_lr_cutoff_bottom5[full_df_endpoints_with_lr_cutoff_bottom5['category'] == 'top5']\n",
    "\n",
    "corr = full_df_endpoints_with_lr_cutoff_bottom5_top5only[['number', 'value', 'params_base_lr', 'params_dim_inner', 'params_dropout', 'params_layers_mp', 'params_layers_post_mp', 'params_layers_pre_mp', 'params_lr_decay', 'params_weight_decay', 'dataset','dataset_size']].corr()\n",
    "sns.heatmap(corr, cmap='icefire', annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for dataset in full_df_no_top_bottom['dataset'].unique():\n",
    "    subset = full_df_endpoints_with_lr_cutoff_bottom5[(full_df_endpoints_with_lr_cutoff_bottom5['dataset'] == dataset) & (full_df_endpoints_with_lr_cutoff_bottom5['category'] == 'top5')]\n",
    "    plt.scatter(subset['dataset_size'].mean(), subset['params_layers_mp'].mean(), s=(dataset_sizes[dataset]/4)**2, label=dataset)\n",
    "    \n",
    "plt.xlabel('Dataset size')\n",
    "plt.ylabel('Mean number of message passing layers')\n",
    "plt.title('Mean message passing layers and dataset size for top 5 hyperparameters')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-heath",
   "metadata": {},
   "source": [
    "### Spread of RMSE values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in full_df_no_top_bottom['dataset'].unique():\n",
    "    subset = full_df_endpoints_with_lr_cutoff_bottom5[(full_df_endpoints_with_lr_cutoff_bottom5['dataset'] == dataset) & (full_df_endpoints_with_lr_cutoff_bottom5['category'] == 'top5')]\n",
    "\n",
    "    sns.histplot(subset['value'], stat='density', label=dataset)\n",
    "        \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-september",
   "metadata": {},
   "source": [
    "### Correlation of layers_mp and dim_inner to size of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for dataset in full_df_no_top_bottom['dataset'].unique():\n",
    "    subset = full_df_endpoints_per_layer_with_lr_cutoff_bottom5[(full_df_endpoints_per_layer_with_lr_cutoff_bottom5['dataset'] == dataset) & (full_df_endpoints_per_layer_with_lr_cutoff_bottom5['category'] == 'top5')]\n",
    "    plt.scatter(subset['params_layers_mp'].mean(), subset['params_dim_inner'].mean(), s=(len(full_df_endpoints_per_layer_with_lr_cutoff_bottom5[(full_df_endpoints_per_layer_with_lr_cutoff_bottom5['dataset'] == dataset)])/2)**2, label=dataset)\n",
    "    \n",
    "plt.xlabel('Mean number of message passing layers')\n",
    "plt.ylabel('Mean number of inner dimensions')\n",
    "plt.title('Mean dimensions and message passing layers for top 5 hyperparameters where size corresponds to dataset size')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for dataset in full_df_no_top_bottom['dataset'].unique():\n",
    "    subset = full_df_endpoints_per_layer_with_lr_cutoff_bottom5[(full_df_endpoints_per_layer_with_lr_cutoff_bottom5['dataset'] == dataset) & (full_df_endpoints_per_layer_with_lr_cutoff_bottom5['category'] == 'top5')]\n",
    "    plt.scatter(subset['params_weight_decay'].mean(), subset['params_dim_inner'].mean(), s=(len(full_df_endpoints_per_layer_with_lr_cutoff_bottom5[(full_df_endpoints_per_layer_with_lr_cutoff_bottom5['dataset'] == dataset)])/2)**2, label=dataset)\n",
    "    \n",
    "plt.xlabel('Mean weight decay')\n",
    "plt.ylabel('Mean number of inner dimensions')\n",
    "plt.title('Mean dimensions and weight decay for top 5 hyperparameters where size corresponds to dataset size')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for dataset in full_df_no_top_bottom['dataset'].unique():\n",
    "    subset = full_df_endpoints_per_layer_with_lr_cutoff_bottom5[(full_df_endpoints_per_layer_with_lr_cutoff_bottom5['dataset'] == dataset) & (full_df_endpoints_per_layer_with_lr_cutoff_bottom5['category'] == 'bottom5')]\n",
    "    plt.scatter(subset['params_layers_mp'].mean(), subset['params_dim_inner'].mean(), s=(len(full_df_endpoints_per_layer_with_lr_cutoff_bottom5[(full_df_endpoints_per_layer_with_lr_cutoff_bottom5['dataset'] == dataset)])/2)**2, label=dataset)\n",
    "    \n",
    "plt.xlabel('Mean number of message passing layers')\n",
    "plt.ylabel('Mean number of inner dimensions')\n",
    "plt.title('Mean dimensions and message passing layers for bottom 5 hyperparameters where size corresponds to dataset size')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for dataset in full_df_no_top_bottom['dataset'].unique():\n",
    "    subset = full_df_no_top_bottom[(full_df_no_top_bottom['dataset'] == dataset)]\n",
    "    plt.scatter(subset['params_layers_mp'].mean(), subset['params_dim_inner'].mean(), s=(len(full_df_no_top_bottom[(full_df_no_top_bottom['dataset'] == dataset)])/4)**2, label=dataset)\n",
    "    \n",
    "plt.xlabel('Mean number of message passing layers')\n",
    "plt.ylabel('Mean number of inner dimensions')\n",
    "plt.title('Mean dimensions and message passing layers for all hyperparameters where size corresponds to dataset size')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_hparams = [\"params_layer_type\", \"params_act\", \"params_attention_type\"]\n",
    "\n",
    "def frac_appearance(df, param_name):\n",
    "    \"\"\"\n",
    "    Generates statistics on fraction of times a discrete parameter appears in a dataset\n",
    "    \"\"\"\n",
    "    dataset_param_frac_dict = {i:{} for i in df[\"dataset\"].tolist()}\n",
    "    \n",
    "    all_disc_param_vals_poss = list(df[param_name].unique())\n",
    "    \n",
    "    params_frac_dict_top5_across_datasets = {i:0 for i in all_disc_param_vals_poss}\n",
    "    params_frac_dict_bottom5_across_datasets = {i:0 for i in all_disc_param_vals_poss}\n",
    "    \n",
    "    for dataset in list(df[\"dataset\"].unique()):\n",
    "        params_frac_dict_top5 = {i:[] for i in all_disc_param_vals_poss}\n",
    "        params_frac_dict_bottom5 = {i:[] for i in all_disc_param_vals_poss}\n",
    "    \n",
    "        top5_appearances_fraction = df[(df[\"dataset\"] == dataset) & (df[\"category\"] == \"top5\")][param_name].tolist()\n",
    "        bottom5_appearances_fraction = df[(df[\"dataset\"] == dataset) & (df[\"category\"] == \"bottom5\")][param_name].tolist()\n",
    "        \n",
    "        for param_val in all_disc_param_vals_poss:\n",
    "            top5_count = top5_appearances_fraction.count(param_val)\n",
    "            params_frac_dict_top5[param_val] = top5_count\n",
    "            \n",
    "            bottom5_count = bottom5_appearances_fraction.count(param_val)\n",
    "            params_frac_dict_bottom5[param_val] = bottom5_count\n",
    "            \n",
    "            params_frac_dict_top5_across_datasets[param_val] += top5_count\n",
    "            params_frac_dict_bottom5_across_datasets[param_val] += bottom5_count\n",
    "            \n",
    "        params_frac_dict_top5 = {k: v for k, v in params_frac_dict_top5.items() if v}\n",
    "        params_frac_dict_bottom5 = {k: v for k, v in params_frac_dict_bottom5.items() if v}\n",
    "            \n",
    "        dataset_param_frac_dict[dataset] = {'top5': params_frac_dict_top5, 'bottom5': params_frac_dict_bottom5}\n",
    "        \n",
    "    return dataset_param_frac_dict, params_frac_dict_top5_across_datasets, params_frac_dict_bottom5_across_datasets\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_param_frac_dict, params_frac_dict_top5_across_datasets, params_frac_dict_bottom5_across_datasets = frac_appearance(df=full_df_endpoints, param_name=\"params_layer_type\")\n",
    "\n",
    "print('top5 across datasets:', params_frac_dict_top5_across_datasets)\n",
    "print('bottom5 across datasets:', params_frac_dict_bottom5_across_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_param_frac_dict, params_frac_dict_top5_across_datasets, params_frac_dict_bottom5_across_datasets = frac_appearance(df=full_df_endpoints, param_name=\"params_act\")\n",
    "\n",
    "print('top5 across datasets:', params_frac_dict_top5_across_datasets)\n",
    "print('bottom5 across datasets:', params_frac_dict_bottom5_across_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_param_frac_dict, params_frac_dict_top5_across_datasets, params_frac_dict_bottom5_across_datasets = frac_appearance(df=full_df_endpoints, param_name=\"params_attention_type\")\n",
    "\n",
    "print('top5 across datasets:', params_frac_dict_top5_across_datasets)\n",
    "print('bottom5 across datasets:', params_frac_dict_bottom5_across_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_param_frac_dict, params_frac_dict_top5_across_datasets, params_frac_dict_bottom5_across_datasets = frac_appearance(df=full_df_endpoints, param_name=\"params_graph_pooling\")\n",
    "\n",
    "print('top5 across datasets:', params_frac_dict_top5_across_datasets)\n",
    "print('bottom5 across datasets:', params_frac_dict_bottom5_across_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_param_frac_dict, params_frac_dict_top5_across_datasets, params_frac_dict_bottom5_across_datasets = frac_appearance(df=full_df_endpoints, param_name=\"params_stage_type\")\n",
    "\n",
    "print('top5 across datasets:', params_frac_dict_top5_across_datasets)\n",
    "print('bottom5 across datasets:', params_frac_dict_bottom5_across_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dataset_with_norm = pd.DataFrame()\n",
    "\n",
    "for dataset in full_df_no_top_bottom['dataset'].unique():\n",
    "    subset_dataset = full_df_no_top_bottom[(full_df_no_top_bottom['dataset'] == dataset)]\n",
    "    dataset_df = datasets[dataset]\n",
    "    \n",
    "    # Get column name containing pIC50 or IC50\n",
    "    col = [i for i in dataset_df.columns if '_mean' in i.lower()]\n",
    "    col = col[0]\n",
    "    \n",
    "    # Mean\n",
    "    mean = dataset_df[col].mean()\n",
    "    # Standard deviation\n",
    "    stddev = dataset_df[col].std()\n",
    "    # 90th percentile\n",
    "    perc_09 = np.percentile(dataset_df[col], 90)\n",
    "    \n",
    "    # Normalize with mean and standard deviation of dataset\n",
    "    subset_dataset['Normalised RMSE'] = (subset_dataset['value'])/perc_09\n",
    "    \n",
    "    dataset_with_norm = pd.concat([dataset_with_norm, subset_dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in full_df_no_top_bottom['dataset'].unique():\n",
    "    subset_top10 = dataset_with_norm[(dataset_with_norm['dataset'] == dataset)].sort_values('Normalised RMSE')[:10]\n",
    "    if subset_top10['Normalised RMSE'].mean()<5:\n",
    "        sns.kdeplot(subset_top10['Normalised RMSE'], label=dataset)\n",
    "        \n",
    "plt.xlabel('RMSE')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(10, 12))\n",
    "\n",
    "axes_indices = [axes[0][0], axes[0][1], axes[0][2], axes[1][0], axes[1][1], axes[1][2], axes[2][0], axes[2][1], axes[2][2]]\n",
    "\n",
    "model_type_colours = {'gatconv':'royalblue', 'ginconv':'darkorange', 'gcnconv':'forestgreen', 'generalsampleedgeconv':'red',\n",
    "       'sageconv':'dimgrey', 'mlp':'brown', 'linear':'hotpink', 'splineconv':'grey', 'generalconv':'darkviolet',\n",
    "       'generaledgeconv':'olive'}\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for idx, dataset in enumerate(dataset_with_norm['dataset'].unique()):\n",
    "    subset_dataset = dataset_with_norm[(dataset_with_norm['dataset'] == dataset) & (dataset_with_norm[\"params_base_lr\"] >= 0.0001)]\n",
    "    \n",
    "    for model_type in dataset_with_norm['params_layer_type'].unique():\n",
    "        if model_type in ['generalsampleedgeconv', 'splineconv', 'generaledgeconv']:\n",
    "            linetype = '-'\n",
    "        else:\n",
    "            linetype = '--'\n",
    "            \n",
    "        subset_dataset_model_type = subset_dataset[subset_dataset['params_layer_type'] == model_type].sort_values('Normalised RMSE')[:10]\n",
    "        \n",
    "        sns.kdeplot(subset_dataset_model_type['Normalised RMSE'], label=model_type, linestyle=linetype, ax=axes_indices[idx], color=model_type_colours[model_type])\n",
    "        \n",
    "        axes_indices[idx].set_xlim(left=0.0)\n",
    "        axes_indices[idx].set_title(dataset, fontsize=10)\n",
    "        axes_indices[idx].set_xlabel(xlabel='')\n",
    "        axes_indices[idx].set_ylabel(ylabel='')\n",
    "        \n",
    "        if model_type not in labels_dict:\n",
    "            handles, labels = axes_indices[idx].get_legend_handles_labels()\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            \n",
    "            if model_type in by_label:\n",
    "                labels_dict[model_type] = by_label[model_type]\n",
    "        \n",
    "fig.legend(labels_dict.values(), labels_dict.keys(), loc='upper right', bbox_to_anchor=(1.22, 0.98), frameon=False)\n",
    "\n",
    "fig.supxlabel('RMSE/90th percentile')\n",
    "fig.supylabel('Density')\n",
    "fig.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_graphgym)",
   "language": "python",
   "name": "conda_graphgym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
